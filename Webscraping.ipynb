{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Web Scraping - Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could scrap some web pages by using beautiful soup. But, we need to define which element in html should we extract.\n",
    "\n",
    "Besides HTML, this library also can be used to parse XML.\n",
    "\n",
    "Visit this link for further information, https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install beautiful soup\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping only for one page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'https://www.toyota.astra.co.id/toyota-connect/news/5-alasan-toyota-calya-begitu-sempurna-di-mata-pemiliknya'\n",
    "\n",
    "#define html parser from beautiful soup\n",
    "response = requests.get(doc, timeout = 10)\n",
    "soup = bs(response.content, \"html.parser\")\n",
    "\n",
    "#get the headline element\n",
    "title = soup.find('h1', attrs = {'class': 'headline'})\n",
    "#clean the headline\n",
    "title = title.text.strip()\n",
    "\n",
    "#get the content\n",
    "content = [element.text for element in soup.find_all('p', {'style': 'margin-left:0cm; margin-right:0cm'})]\n",
    "    \n",
    "print('title: ', title)\n",
    "print('content: ', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's scrape some pages from the website!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_docs = ['https://www.toyota.astra.co.id/toyota-connect/news/5-alasan-toyota-calya-begitu-sempurna-di-mata-pemiliknya',\n",
    "            'https://www.toyota.astra.co.id/toyota-connect/news/sejarah-toyota-gr-supra-sang-legenda-sportcar-yang-jadi-primadona-di-giias-2019',\n",
    "            'https://www.toyota.astra.co.id/toyota-connect/news/keren-ternyata-youtuber-zach-king-pecinta-toyota-prius',\n",
    "            'https://www.toyota.astra.co.id/toyota-connect/news/5-kelebihan-yang-bikin-toyota-gr-supra-jadi-pusat-perhatian-di-giias-2019']\n",
    "\n",
    "#extract documents and store in one array\n",
    "docs = []\n",
    "for index, document in enumerate(html_docs):\n",
    "    \n",
    "    #prepare a dictionary for each page\n",
    "    doc = {}\n",
    "    #define html parser from beautiful soup\n",
    "    response = requests.get(document, timeout = 10)\n",
    "    soup = bs(response.content, \"html.parser\")\n",
    "    \n",
    "    #clean the text\n",
    "    title = soup.find('h1', attrs = {'class': 'headline'})\n",
    "    doc['id'] = index + 1\n",
    "    doc['title'] = title.text.strip() #clean the text\n",
    "    \n",
    "    #store all the rows within p element\n",
    "    content = [element.text for element in soup.find_all('p', {'style': 'margin-left:0cm; margin-right:0cm'})]\n",
    "    #remove unwanted lines\n",
    "    doc['content'] = ''.join(element for element in content)\n",
    "    \n",
    "    docs.append(doc)\n",
    "    \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We cannot extract document id 3 and 4!\n",
    "What is happening on this part?\n",
    "Let's see the web structure of those pages (id 3 and 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract documents and store in one array\n",
    "docs = []\n",
    "for index, document in enumerate(html_docs):\n",
    "    \n",
    "    #prepare a dictionary for each page\n",
    "    doc = {}\n",
    "    #define html parser from beautiful soup\n",
    "    response = requests.get(document, timeout = 10)\n",
    "    soup = bs(response.content, \"html.parser\")\n",
    "    \n",
    "    #get the title\n",
    "    title = soup.find('h1', attrs = {'class': 'headline'})\n",
    "    doc['id'] = index + 1\n",
    "    #clean the text\n",
    "    doc['title'] = title.text.strip()\n",
    "    \n",
    "    #store all the rows within p element\n",
    "    content = [element.text for element in soup.find_all('p')]\n",
    "    #remove unwanted lines\n",
    "    del content[0:2]\n",
    "    #merge all element in a list\n",
    "    doc['content'] = ''.join(element for element in content)\n",
    "    \n",
    "    docs.append(doc)\n",
    "    \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store it to dataframe\n",
    "In order to create better presentation of our data, we could store the data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe from the above list\n",
    "data = pd.DataFrame(docs, columns = ['id', 'title', 'content'])\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
